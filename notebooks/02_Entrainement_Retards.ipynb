{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3682aa28-2f03-4005-bd84-ae5959a140f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib # ou pickle\n",
    "\n",
    "# --- BLOC DE CONFIGURATION DES CHEMINS (À METTRE AU DÉBUT) ---\n",
    "\n",
    "# Le chemin vers le dossier 'models' est UN NIVEAU AU-DESSUS (../) du dossier 'notebooks'\n",
    "MODELS_DIR = '../models'\n",
    "\n",
    "# On s'assure que ce dossier existe. S'il n'existe pas, on le crée.\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b14cc78-1a9e-4c0e-9315-49c6dd6bb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion à la base de données établie avec succès !\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CELLULE 1 : Imports et Configuration de la connexion à la BDD\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de la connexion à la base de données (celle du service Docker)\n",
    "# IMPORTANT: 'host.docker.internal' permet au notebook (tournant localement) \n",
    "# de voir la BDD qui est dans le conteneur Docker.\n",
    "DB_USER = 'root'\n",
    "DB_PASSWORD = 'root'\n",
    "DB_HOST = 'localhost' # ou 'host.docker.internal' si le notebook tourne en dehors de Docker\n",
    "DB_PORT = '3306'\n",
    "DB_NAME = 'sicda_easytime'\n",
    "\n",
    "connection_str = f\"mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(connection_str)\n",
    "\n",
    "print(\"Connexion à la base de données établie avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f5eff9-2183-401f-80cc-777c358d927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées et standardisées avec succès.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CELLULE 2 : Chargement et Standardisation des Données (VERSION DÉFINITIVE)\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    # On charge les tables brutes\n",
    "    df_mvt_raw = pd.read_sql(\"SELECT * FROM mouvement\", engine)\n",
    "    df_users_raw = pd.read_sql(\"SELECT * FROM utilisateur\", engine)\n",
    "    df_jour_raw = pd.read_sql(\"SELECT * FROM jour\", engine)\n",
    "    df_plage_horaire_raw = pd.read_sql(\"SELECT * FROM plage_horaire\", engine)\n",
    "\n",
    "    # --- ÉTAPE DE NETTOYAGE ET RENOMMAGE ---\n",
    "    # On travaille sur des copies pour garder les originaux intacts\n",
    "    \n",
    "    # Pour Mouvement\n",
    "    df_mvt = df_mvt_raw.copy()\n",
    "    df_mvt.rename(columns={'ID': 'mvt_id', 'BADGE': 'badge', 'DATE_MOUV': 'date_mouv'}, inplace=True)\n",
    "    df_mvt['date_mouv'] = pd.to_datetime(df_mvt['date_mouv'])\n",
    "    \n",
    "    # Pour Utilisateur\n",
    "    df_users = df_users_raw.copy()\n",
    "    df_users.rename(columns={'ID': 'user_id', 'BADGE': 'badge', 'PLANNING_FK': 'planning_fk'}, inplace=True)\n",
    "    \n",
    "    # Pour Jour\n",
    "    df_jour = df_jour_raw.copy()\n",
    "    df_jour.rename(columns={'ID': 'jour_id', 'PLANNING_HEBDO_FK': 'planning_id', 'LIBELLE': 'jour_nom', 'HORAIRE_FK': 'horaire_id'}, inplace=True)\n",
    "    \n",
    "    # Pour Plage Horaire\n",
    "    df_plage_horaire = df_plage_horaire_raw.copy()\n",
    "    df_plage_horaire.rename(columns={'ID': 'plage_id', 'SMPL_HORAIRE_FK': 'horaire_id', 'DEBUT': 'heure_debut_theorique_double', 'TOL_ENTREE': 'tolerance_entree'}, inplace=True)\n",
    "    \n",
    "    print(\"Données chargées et standardisées avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement ou de la standardisation des données : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c692808-f745-4443-8d56-cc5f16f20ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'analyse pour trouver les retards réels...\n",
      "Analyse terminée. 69 retards réels ont été identifiés.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>badge</th>\n",
       "      <th>jour_date</th>\n",
       "      <th>duree_retard_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-21</td>\n",
       "      <td>44.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>10.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-25</td>\n",
       "      <td>3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>37.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  badge   jour_date  duree_retard_minutes\n",
       "0     1  2016-09-12             62.000000\n",
       "2     1  2023-01-21             44.283333\n",
       "4     1  2023-01-24             10.033333\n",
       "5     1  2023-01-25              3.700000\n",
       "7     1  2023-01-30             37.100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CELLULE 3 : Logique de Détection des Retards Réels (VERSION DÉFINITIVE)\n",
    "# ====================================================================\n",
    "print(\"Début de l'analyse pour trouver les retards réels...\")\n",
    "\n",
    "# 1. Joindre pointages et utilisateurs (maintenant les colonnes 'badge' correspondent)\n",
    "df_merged = pd.merge(df_mvt, df_users[['badge', 'planning_fk']], on='badge', how='left')\n",
    "df_merged.dropna(subset=['planning_fk'], inplace=True)\n",
    "df_merged['planning_fk'] = df_merged['planning_fk'].astype(int)\n",
    "\n",
    "# 2. Extraire la date et le nom du jour de la semaine\n",
    "df_merged['jour_date'] = df_merged['date_mouv'].dt.date\n",
    "day_map_french = {0: 'Lundi', 1: 'Mardi', 2: 'Mercredi', 3: 'Jeudi', 4: 'Vendredi', 5: 'Samedi', 6: 'Dimanche'}\n",
    "df_merged['jour_nom'] = df_merged['date_mouv'].dt.dayofweek.map(day_map_french)\n",
    "\n",
    "# 3. Ne garder que le premier pointage de chaque employé pour chaque jour\n",
    "df_arrivées = df_merged.loc[df_merged.groupby(['badge', 'jour_date'])['date_mouv'].idxmin()].copy()\n",
    "\n",
    "# 4. Chaîne de jointures\n",
    "# 4.1. Joindre avec 'jour' pour trouver l'ID de l'horaire\n",
    "df_step1 = pd.merge(\n",
    "    df_arrivées, \n",
    "    df_jour[['planning_id', 'jour_nom', 'horaire_id']], \n",
    "    left_on=['planning_fk', 'jour_nom'], \n",
    "    right_on=['planning_id', 'jour_nom'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4.2. Joindre avec 'plage_horaire'\n",
    "df_final_join = pd.merge(\n",
    "    df_step1, \n",
    "    df_plage_horaire[['horaire_id', 'heure_debut_theorique_double', 'tolerance_entree']],\n",
    "    on='horaire_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5. Nettoyer\n",
    "df_final_join.dropna(subset=['heure_debut_theorique_double'], inplace=True)\n",
    "\n",
    "# 6. Calculer la durée du retard\n",
    "df_final_join['heure_debut_theorique'] = pd.to_datetime(df_final_join['heure_debut_theorique_double'], unit='h').dt.time\n",
    "df_final_join['heure_arrivee'] = df_final_join['date_mouv'].dt.time\n",
    "df_final_join['duree_retard_minutes'] = (\n",
    "    (pd.to_datetime(df_final_join['heure_arrivee'].astype(str)) - \n",
    "     pd.to_datetime(df_final_join['heure_debut_theorique'].astype(str))).dt.total_seconds() / 60\n",
    ")\n",
    "\n",
    "# 7. Appliquer la tolérance\n",
    "df_final_join['duree_retard_minutes'] -= df_final_join['tolerance_entree'].fillna(0)\n",
    "df_final_join['est_en_retard'] = df_final_join['duree_retard_minutes'] > 0\n",
    "\n",
    "# 8. Créer le dataset final\n",
    "retards_dataset = df_final_join[df_final_join['est_en_retard']].copy()\n",
    "retards_dataset = retards_dataset[['badge', 'jour_date', 'duree_retard_minutes']]\n",
    "\n",
    "print(f\"Analyse terminée. {len(retards_dataset)} retards réels ont été identifiés.\")\n",
    "retards_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b7152d-d50c-4253-ae56-35a2dcb20fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création du dataset d'entraînement final...\n",
      "Dataset d'entraînement 'retards_data.csv' créé avec 69 exemples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duree_retard_minutes</th>\n",
       "      <th>nb_retards_mois_precedent</th>\n",
       "      <th>est_debut_semaine</th>\n",
       "      <th>charge_travail_equipe_jour</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.283333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.033333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.700000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37.100000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duree_retard_minutes  nb_retards_mois_precedent  est_debut_semaine  \\\n",
       "0             62.000000                          0                  1   \n",
       "2             44.283333                          1                  0   \n",
       "4             10.033333                          2                  0   \n",
       "5              3.700000                          3                  0   \n",
       "7             37.100000                          4                  1   \n",
       "\n",
       "   charge_travail_equipe_jour  action  \n",
       "0                        0.83       0  \n",
       "2                        0.31       0  \n",
       "4                        0.31       0  \n",
       "5                        0.44       1  \n",
       "7                        0.84       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CELLULE 4 : Création du Dataset d'Entraînement avec Features\n",
    "# ====================================================================\n",
    "print(\"Création du dataset d'entraînement final...\")\n",
    "\n",
    "# On crée une colonne cible simulée : 'action' (1=TOLERER, 0=JUSTIFICATION REQUISE)\n",
    "# Logique de simulation : On tolère plus facilement les petits retards, en début de semaine, \n",
    "# et si l'employé est habituellement ponctuel.\n",
    "conditions = [\n",
    "    (retards_dataset['duree_retard_minutes'] <= 10),\n",
    "    (retards_dataset['duree_retard_minutes'] > 30)\n",
    "]\n",
    "choices = [1, 0] # Tolérer les petits, ne pas tolérer les gros\n",
    "retards_dataset['action'] = np.select(conditions, choices, default=np.random.choice([0, 1], size=len(retards_dataset), p=[0.6, 0.4]))\n",
    "\n",
    "\n",
    "# Création des features pour le modèle\n",
    "retards_dataset['jour_date'] = pd.to_datetime(retards_dataset['jour_date'])\n",
    "retards_dataset['est_debut_semaine'] = (retards_dataset['jour_date'].dt.dayofweek == 0).astype(int) # Lundi = 1, sinon 0\n",
    "\n",
    "# Feature : Nombre de retards de cet employé dans les 30 derniers jours\n",
    "# C'est une opération coûteuse, on la fait de manière optimisée\n",
    "retards_dataset = retards_dataset.sort_values(by=['badge', 'jour_date'])\n",
    "nb_retards_par_badge = retards_dataset.groupby('badge').cumcount()\n",
    "retards_dataset['nb_retards_mois_precedent'] = nb_retards_par_badge\n",
    "\n",
    "# Feature : Charge de travail (simulée pour l'exemple)\n",
    "retards_dataset['charge_travail_equipe_jour'] = np.random.uniform(0.3, 0.9, size=len(retards_dataset)).round(2)\n",
    "\n",
    "# Sélection des colonnes finales pour le dataset\n",
    "final_dataset = retards_dataset[[\n",
    "    'duree_retard_minutes',\n",
    "    'nb_retards_mois_precedent',\n",
    "    'est_debut_semaine',\n",
    "    'charge_travail_equipe_jour',\n",
    "    'action'\n",
    "]].copy()\n",
    "\n",
    "# Enregistrer le dataset pour analyse future\n",
    "final_dataset.to_csv('retards_data.csv', index=False)\n",
    "\n",
    "print(f\"Dataset d'entraînement 'retards_data.csv' créé avec {len(final_dataset)} exemples.\")\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63279980-6ff6-455d-bc16-cc4fd9fc8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du nouveau modèle pour les retards...\n",
      "\n",
      "--- Rapport de Classification ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         6\n",
      "           1       0.73      1.00      0.84         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.86      0.75      0.75        14\n",
      "weighted avg       0.84      0.79      0.77        14\n",
      "\n",
      "Accuracy: 0.79\n",
      "\n",
      "Modèle entraîné et sauvegardé avec succès dans : '../models\\model_retards_contextuel.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CELLULE 5 : Entraînement et Sauvegarde du Modèle\n",
    "# ====================================================================\n",
    "print(\"Entraînement du nouveau modèle pour les retards...\")\n",
    "\n",
    "# 1. Préparer les données\n",
    "X = final_dataset.drop('action', axis=1)\n",
    "y = final_dataset['action']\n",
    "\n",
    "# 2. Diviser en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Initialiser et entraîner le modèle\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Évaluer le modèle\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\n--- Rapport de Classification ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# 5. Sauvegarder le modèle entraîné\n",
    "model_path = os.path.join(MODELS_DIR, 'model_retards_contextuel.pkl')\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"\\nModèle entraîné et sauvegardé avec succès dans : '{model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd8035-3837-443c-af66-219d3b0b4249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
